ArteVis (2025)
Description:
For each source image, ArteVis trains a separate fully connected feed-forward neural network (3 hidden layers × 200 neurons) to learn the image as a continuous function: given a pixel coordinate, the network outputs the corresponding color value. Training uses the hyperbolic tangent activation (tanh) and the Adam optimizer; the image itself serves as the loss target.
The learned weight matrices that connect the layers are visualized, very large weights appear red, very small weights blue, and values near zero white. After training, these matrices are rotated, inverted, and permuted. The network no longer reconstructs its source but instead generates unfamiliar, abstract images that retain little if any trace of the original. ArteVis exposes how an image can become a function, and how small structural interventions in that function open a space between representation and abstraction.

Technical notes:
Architecture: 3 hidden layers, 200 neurons each · Activation: tanh · Optimizer: Adam · One network per image (independent training) · Output: pixel value at a given coordinate · Weight-magnitude color map: red (high) / white (≈0) / blue (low)

ArteVis (2025)
Beschreibung:
Für jedes Ausgangsbild trainiert ArteVis ein eigenes, vollständig verbundenes Feed-Forward-Netzwerk (3 verdeckte Schichten × 200 Neuronen). Das Bild wird als kontinuierliche Funktion erlernt: Zu einer Pixelkoordinate gibt das Netz den entsprechenden Farbwert aus. Trainiert wird mit der Aktivierungsfunktion tanh (hyperbolischer Tangens) und dem Optimierer Adam; das Bild selbst dient als Verlustziel.
Die gelernten Gewichtsmatrizen zwischen den Schichten werden sichtbar gemacht—sehr große Gewichte erscheinen rot, sehr kleine blau, Werte nahe null weiß. Nach dem Training werden diese Matrizen rotiert, invertiert und vertauscht. Das Netz rekonstruiert dann nicht mehr sein Ausgangsbild, sondern erzeugt ungewohnte, abstrakte Bilder, die kaum Merkmale des Originals bewahren. ArteVis zeigt, wie ein Bild zur Funktion werden kann—und wie kleine Eingriffe in deren innere Struktur einen Raum zwischen Darstellung und Abstraktion eröffnen.
Technische Angaben

Architektur: 3 Hidden-Layer, je 200 Neuronen · Aktivierung: tanh · Optimierer: Adam · Ein Netz pro Bild (separates Training) · Ausgabe: Pixelwert an gegebener Koordinate · Farbzuordnung der Gewichtsbeträge: rot (hoch) / weiß (≈0) / blau (niedrig)
